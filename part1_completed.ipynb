{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WIN Hackathon 2 - Computational modelling of behaviour - Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN0OKdOSQ1cd"
      },
      "source": [
        "**Developers**\n",
        "\n",
        "Jan Grohn (jan.grohn@psy.ox.ac.uk), Miriam Klein-Flügge (miriam.klein-flugge@psy.ox.ac.uk)  \n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Many experiments in learning and decision making require the estimation of values and how these change over time. Learning which stimuli and actions have higher value allows humans and other animals to select which stimuli to approach or avoid, and which actions to take.\n",
        "\n",
        "Some of the most commonly used models of learning and reinforcement were first developed by **Bush and Mosteller** in the 1950s, and further elaborated on by **Rescorla and Wagner** in the 1970s.\n",
        "\n",
        "They have since developed into the field of **Reinforcement Learning** in computer science.\n",
        "\n",
        "*Our aims for part 1 are:*\n",
        "\n",
        "1.  understand an experimental paradigm of learning and decision making\n",
        "2.  get to know a simple reinforcement learning model that can be used to explain behaviour in this task\n",
        "3.  understand how changing parameters in this model affects its behaviour\n",
        "4.  compute the subjective value of choice options\n",
        "\n",
        "Note  that  the  final  section  (marked  ***)  is  a  slightly  mathsy optional section.  It’s  not  essential  to understand  it  to  progress  with  the  later  parts  in  the  hackathon.  However,  it  provides  a theoretical foundation for the analyses that you’ll do in later practicals and can be a fun\n",
        "exercise to do!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries and set global parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeEaQjFUYxCP"
      },
      "outputs": [],
      "source": [
        "# check if we are running on colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    _ON_COLAB = True\n",
        "except:\n",
        "    _ON_COLAB = False\n",
        "\n",
        "if _ON_COLAB:\n",
        "    # this allows us to make interactive figures\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "    # load in some custom functions for this hackathon\n",
        "    !rm -r *\n",
        "    !git clone -b winhack https://github.com/jangrohn/ComputationalmodelingBlockPractical\n",
        "    !cp -R ComputationalmodelingBlockPractical/part1/ part1\n",
        "    !rm -rf ComputationalmodelingBlockPractical\n",
        "\n",
        "    %pip install -r ./part1/requirements.txt\n",
        "\n",
        "import numpy as np\n",
        "rng = np.random.default_rng(12345)\n",
        "from part1 import plot_schedule, plot_interactive_RL_model, visualise_utility_function, plot_RL_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWiW8-gRvDiz"
      },
      "source": [
        "The last couple of lines of code in the previous cell load in some custom functions we wrote for this hackathon, which we will be using throughout this session. If you're interested in examining the code further, you can find them at https://github.com/JanGrohn/ComputationalModelingBlockPractical or by clicking the folder icon in the sidebar to the left and look through the scripts there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWLm4LvgS-KE"
      },
      "source": [
        "# Section 1: Running a simple experiment\n",
        "\n",
        "You can try out the task we will be analysing in this hackathon at  https://jangrohn.github.io/volatility_study/. If you would like, play through the task as best as you can to get a feel for the study. At the beginning of the task, initially choose one of the two conditions randomly but if you have the time also restart the task and play the other condition afterwards. The task doesn't save any of your data, but you will have an option to download it once you're finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VFNvxBrTuQ9"
      },
      "source": [
        "# Section 2: Coding a simple reinforcement learning model in Python, and understanding the effects of varying learning rates\n",
        "\n",
        "Here are some questions that you might want to discuss with your neighbours. Thinking through these questions will help you develop an intuition for what we will model later on.\n",
        "\n",
        "*How did you learn whether green or orange was more likely to be rewarded?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rKNVSJsUHod"
      },
      "source": [
        "*How did you weigh this up against the number of points available when making your choices?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ9VSJT_I_41"
      },
      "source": [
        "*What do you think was the difference between condition 1 and condition 2 in the task?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-EC_2RUUPzH"
      },
      "source": [
        "*Why do you think we included points – rather than simply letting participants learn which option has a higher reward probability?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAMps7idUyer"
      },
      "source": [
        "Reinforcement  learning  models  provide  a  way  of  tracking  the  probabilities  of  different outcomes when different actions are taken.\n",
        "\n",
        "In this task, one of the key problems is to estimate *the probability that green is to be rewarded* on each trial. (Note that this is the *same* as 1 minus the probability that blue will be rewarded – so we only need to track one probability).\n",
        "\n",
        "We can learn this probability by calculating a *prediction error* on each trial:\n",
        "\n",
        "$$\n",
        "\\underbrace{\\delta_t}_\\textrm{prediction error} = \\underbrace{o_t}_\\textrm{outcome} - \\underbrace{p_t}_\\textrm{model prediction} \\tag{Equation 1}\n",
        "$$\n",
        "\n",
        "where $\\delta_t$ is the prediction error on trial $t$, $o_t$ is the outcome (1 if green was rewarded, 0 if blue was rewarded) on that trial, and $p_t$ is the current estimate of the probability that green will be rewarded (this is called `probOpt1` in the Python code). Have a think about what a prediction error might look like on a trial that does give reward and on a trial where no reward is obtained. What sign does it take in each case?\n",
        "\n",
        "We then use this prediction error to *update our expectation* of how likely it is that green will be rewarded in the future.\n",
        "\n",
        "$$\n",
        "\\underbrace{p_{t+1}}_\\textrm{new prediction} = \\underbrace{p_t}_\\textrm{old prediction} + \\underbrace{\\alpha \\delta_t}_\\textrm{scaled prediction error} \\tag{Equation 2}\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a parameter called the *learning  rate*, whose value is $0 < \\alpha \\leq 1$. In your head, simulate different scenarios of this equation with a large (1) or small (0.1) learning rate and a positive or negative prediction error. What does the update look like, how does it differ?\n",
        "\n",
        "The learning rate sets the *speed* at which the model learns from previous experience. Let’s explore directly in Python what happens when we vary the learning rate.\n",
        "\n",
        "## Learning rates, and how they affect probability learning\n",
        "\n",
        "The next code cell sets up, and plots, a simple ‘reward schedule’ where the true probability of green being rewarded is fixed at 0.8. Read through this code and try to understand what it is doing.\n",
        "\n",
        "Once you have read through the code, run it to produce the figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning with a fixed schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8u5lMFUUGAk"
      },
      "outputs": [],
      "source": [
        "def generate_schedule(trueProbability, rng = rng):\n",
        "  '''\n",
        "  Returns if option 1 (1) or option 2 (0) is rewarded on a trial\n",
        "\n",
        "    Parameters:\n",
        "        trueProbability(float array): The probability with which option 1 is\n",
        "          rewarded on each trial\n",
        "        rng (numpy random number generator, defaults to rng)\n",
        "\n",
        "    Returns:\n",
        "        opt1rewarded(int array): 1 if option 1 is rewarded on a trial, 0 if\n",
        "          option 2 is rewarded on a trial\n",
        "  '''\n",
        "  # We'll simulate whether opt 1 was rewarded on every trial. For each trial, we\n",
        "  # first generate a uniformly distributed random number between 0 and 1.\n",
        "  randomNumbers = rng.random(len(trueProbability))\n",
        "\n",
        "  # The trial is rewarded if that number is smaller than trueProbability, and\n",
        "  # unrewarded otherwise.\n",
        "  opt1Rewarded = randomNumbers < trueProbability\n",
        "\n",
        "  # We return the outcome of this comparison (which is either True or False for\n",
        "  # each trial) an an integer (which is 0 or 1 for each trial).\n",
        "  return opt1Rewarded.astype(int)\n",
        "\n",
        "# this is the true probability that green is rewarded\n",
        "fixedProb = 0.8\n",
        "\n",
        "# this is the number of trials\n",
        "nTrials = 200\n",
        "\n",
        "# reward probability on each trial\n",
        "trueProbability = np.ones(nTrials, dtype = float) * fixedProb\n",
        "\n",
        "# generate outcomes on each trial\n",
        "opt1Rewarded = generate_schedule(trueProbability)\n",
        "\n",
        "# visualise which option was rewarded on each trial\n",
        "plot_schedule(opt1Rewarded, trueProbability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjnZma4urfqS"
      },
      "source": [
        "The dots are at 1 every time that green is rewarded, and at 0 every time that blue is rewarded. The black dotted line is the true probability of reward (which the subject doesn’t know in the experiment).\n",
        "\n",
        "The function RL_model defined in the next cell takes as its input: whether green is rewarded on each trial (`opt1rewarded`), what the model’s learning rate $\\alpha$ is set to, and what the starting probability on the first trial is ($p_1$). It tries to return `probOpt1`, the probability of green being rewarded, as its output. However, the final two equations in the function\n",
        "haven’t been completed. Open the next cell and complete the missing lines of code.\n",
        "\n",
        "Once you’ve done this, you should be able to run this cell without receiving any errors, and the figure should now have a red trace that approaches the true probability:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating the RL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ks3qNcHTuAp"
      },
      "outputs": [],
      "source": [
        "def RL_model(opt1Rewarded, alpha, startingProb = 0.5):\n",
        "  '''\n",
        "  Returns how likely option 1 is rewarded on each trial.\n",
        "\n",
        "    Parameters:\n",
        "        opt1rewarded(bool array): True if option 1 is rewarded on a trial, False\n",
        "          if option 2 is rewarded on a trial.\n",
        "        alpha(float): fixed learning rate, greater than 0, less than/equal to 1\n",
        "        startingProb(float): starting probability (defaults to 0.5).\n",
        "\n",
        "    Returns:\n",
        "        probOpt1(float array): how likely option 1 is rewarded on each trial\n",
        "          according to the RL model.\n",
        "  '''\n",
        "\n",
        "  # check that alpha has been set appropriately\n",
        "  assert alpha > 0, 'Learning rate (alpha) must be greater than 0'\n",
        "  assert alpha <= 1,'Learning rate (alpha) must be less than or equal to 1'\n",
        "\n",
        "  # check that startingProb has been set appropriately\n",
        "  assert startingProb >= 0, 'startingProb must be greater or equal than 0'\n",
        "  assert startingProb <= 1, 'startingProb must be less than or equal to 1'\n",
        "\n",
        "  # calculate the number of trials\n",
        "  nTrials = len(opt1Rewarded)\n",
        "\n",
        "  # pre-create some vectors we're going to assign into\n",
        "  probOpt1 = np.zeros(nTrials, dtype = float)\n",
        "  delta    = np.zeros(nTrials, dtype = float)\n",
        "\n",
        "  # set the first trial's prediction to be equal to the starting probability\n",
        "  probOpt1[0] = startingProb\n",
        "\n",
        "  # solution:\n",
        "  for t in range(nTrials-1):\n",
        "        delta[t] = opt1Rewarded[t] - probOpt1[t]\n",
        "        probOpt1[t+1] = probOpt1[t] + alpha*delta[t]\n",
        "\n",
        "  return probOpt1\n",
        "\n",
        "# this defines the model's estimated pronbabilty on the very first trial\n",
        "startingProb = 0.5\n",
        "\n",
        "# this is the model's learning rate\n",
        "alpha = 0.05\n",
        "\n",
        "# run the RL model\n",
        "probOpt1 = RL_model(opt1Rewarded, alpha, startingProb)\n",
        "\n",
        "plot_schedule(opt1Rewarded, trueProbability, probOpt1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7m8qijvtwIf"
      },
      "source": [
        "Now  try  playing  around  with  the  three  parameters `trueProb` `startingProb`, and `alpha`. Particularly try to understand the effects of varying `alpha`. What are the advantages of having a low $\\alpha$? What are the advantages of having a high $\\alpha$? If you set $\\alpha$ to 1, how does the model behave? You can change the value using the sliders below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzLmPt6oAiDX"
      },
      "outputs": [],
      "source": [
        "plot_interactive_RL_model(opt1Rewarded, trueProbability, RL_model, generate_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14dSaEmzyQA8"
      },
      "source": [
        "## Using a reversal schedule\n",
        "In the experiment you performed earlier, the reward probability isn’t fixed, but it reverses at  various  points  during  the  experiment  –  so  at  some  points  green  is  more  likely  to  be rewarded, but at other points blue is more likely to be rewarded.\n",
        "\n",
        "Let’s now generate such a schedule during which reversals take\n",
        "place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regenerating the schedule and the RL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iHpftBxnCB83"
      },
      "outputs": [],
      "source": [
        "# now we use a schedule with some reversals\n",
        "trueProbability = np.concatenate((np.ones(25, dtype = float)*0.25,\n",
        "                                  np.ones(25, dtype = float)*0.75,\n",
        "                                  np.ones(25, dtype = float)*0.25,\n",
        "                                  np.ones(25, dtype = float)*0.75,\n",
        "                                  np.ones(100, dtype = float)*0.25))\n",
        "\n",
        "opt1Rewarded = generate_schedule(trueProbability)\n",
        "\n",
        "plot_interactive_RL_model(opt1Rewarded, trueProbability, RL_model, generate_schedule, change_trueProb = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_KpGykJ06c0"
      },
      "source": [
        "Change the parameters above to see whether your reinforcement learner can keep track of this changing probability.\n",
        "\n",
        "In the above schedule, there are periods where the reward environment is volatile (it reverses quite frequently) and other periods where the reward environment is stable (it doesn't change very frequently). Our hypothesis  is  that  people  adjust  their  learning  rates  depending  upon  the  stability  of  the environment.\n",
        "\n",
        "Let's assume that subjects want to get their estimated probability as close to the true probability as possible. Based upon what you learnt about varying $\\alpha$, in which environment might it be helpful to have a lower $\\alpha$? In which environment might it be helpful to have a higher $\\alpha$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDmHw4CKlP4h"
      },
      "source": [
        "The reasoning behind why you might need to have a different $\\alpha$ in different situations is discussed more fully in the following paper:\n",
        "\n",
        "Behrens, T. E. J., Woolrich, M. W., Walton, M. E., & Rushworth, M. F. S. (2007). Learning the  value  of  information  in  an  uncertain  world,  Nature  Neuroscience  10(9),  1214–1221. http://doi.org/10.1038/nn1954"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The computational model above is applied to every trial of the experiment. So far, it can make predictions about how likely a reward is to be hidden behind option 1 or option 2. However, the model does not yet specify which option would be a better choice as it currently still ignores that there are different reward magnitudes (number of points) associated with each option on each trial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integrating reward magnitude and probability: utility\n",
        "\n",
        "We need to now specify how the model combines the two dimensions to derive an overall utility (also sometimes called 'value') for each option.\n",
        "\n",
        "Can you think of at least two different ways according to which participants might combine reward probabilities and magnitudes to figure out which option is better? Which of the different ways you have suggested is better or worse? Maybe think back to how you played the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative utility\n",
        "\n",
        "As an example for why you need to combine the two dimensions, imagine that you are almost 100% sure that you will receive a reward if you choose an option, but if the reward is very small (say 2 out of 100 possible points), you might nevertheless not want to pick the option. We formalize this in equation 3:\n",
        "\n",
        "$$\n",
        "\\underbrace{u}_\\textrm{utility} = \\underbrace{m}_\\textrm{reward magnitude} \\times \\underbrace{p}_\\textrm{reward probabilty} \\tag{Equation 3}\n",
        "$$\n",
        "\n",
        "Here, we define the ultity $u$ as the product of the reward probability $p$ and magnitude $m$. We can refer to this way of calculating utility as 'multiplicative utility'. We define and plot this utility function in the next code cell.\n",
        "\n",
        "Complete the utility function in the next code cell using equation 3. Then run the cell. The 3d plot it produces can be rotated by holding and dragging your cursor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the utility function as per equation 3\n",
        "def multiplicative_utility(mag, prob):\n",
        "  return mag*prob\n",
        "\n",
        "visualise_utility_function(multiplicative_utility)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Describe why the utility function in equation 3 produces utility of the shape shown in the above plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additive utility\n",
        "\n",
        "As another example, imagine that you consider reward magnitude and probability separately from each other. This is formalised in equation 4:\n",
        "\n",
        "$$\n",
        "\\underbrace{u}_\\textrm{utility} = \\overbrace{\\omega}^\\textrm{magnitude weight} \\times \\underbrace{m}_\\textrm{reward magnitude} + \\overbrace{(1-\\omega)}^\\textrm{probability weight} \\times \\underbrace{p}_\\textrm{reward probabilty} \\tag{Equation 4}\n",
        "$$\n",
        "\n",
        "Here, reward magintude and probability are added together, which is why we refer to this type of utility as 'additive utility'. We weight up the two terms of the sum by a weight $\\omega$, which is $0 \\leq \\omega \\leq 1$. To ensure that probability and magnitude are on the same scale, we assume that the magnitude has been normalised to be on a range from $0$ to $1$ (i.e., it has been divided by $100$).  \n",
        "\n",
        "At the extreme, you might even consider only reward magnitude or only reward probaility when making your choices. This can be modelled by setting $\\omega$ to which values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the next code cell that visualises additive utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# additive utility function as per equation 4\n",
        "def additive_utility(mag, prob, omega):\n",
        "  return (omega*(mag/100) + (1-omega)*prob)*100\n",
        "\n",
        "visualise_utility_function(additive_utility, omega = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the value of omega using the slider to see how it affects the shape of the utility function. Describe why the utility function in equation 4 produces utility of the shape shown in the above plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What are the major differences in the shape of multiplicative and additive utility?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jZjHSuV18zx"
      },
      "source": [
        "# ***Section 3: Understanding learning rates as weighted sums\n",
        "\n",
        "The key insight here is that the learning rate, $\\alpha$, determines how the currently estimated probability on the next trial, $p_{t+1}$, is influenced by the past history of trials. We can think about  this in another way. Look back at equations 1 and 2. $p_{t+1}$ depends upon the outcome of the\n",
        "most recent trial, $o_t$, but also the last trial’s probability estimate, $p_t$. However, $p_t$ could itself be written in terms of the probability of the previous trial’s outcome, $p_{t-1}$, and $p_{t-1}$. In turn $p_{t-1}$ could\n",
        "be written in terms of $o_{t-2}$ and $p_{t-2}$. And so on.\n",
        "\n",
        "In short, it becomes possible, when you are at trial $T$, to think of $p_{T+1}$ as a weighted sum of all the previous outcomes that the model experienced:\n",
        "\n",
        "$$\n",
        "p_{T+1} = (1-\\alpha)^T p_1 + \\sum^T_{t=1} w_t o_t \\tag{Equation 5}\n",
        "$$\n",
        "\n",
        "Where $p_1$ is the starting probability (this becomes less and less important as we get further away from it, as $(1-\\alpha)^T$ shrinks to zero), and $w_t$ is the amount of weight given to the outcome on trial $t$ on the current trial:\n",
        "\n",
        "$$\n",
        "w_t = \\alpha(1-\\alpha)^{T-t} \\tag{Equation 6}\n",
        "$$\n",
        "\n",
        "$T$ is the current trial number, and so $T-t$ indexes how many trials into the past we are looking. In the final exercise, we see if you can derive this equation, step- by-step.\n",
        "\n",
        "What does $w_t$ actually look like? This is explored in the next cell, which plots equation 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk6Rp5DcFnmt"
      },
      "outputs": [],
      "source": [
        "plot_RL_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPRC8pDX67Iz"
      },
      "source": [
        "This plot is also discussed at the beginning of the following book chapter (which is available as a PDF online at http://www.princeton.edu/~ndaw/dt.pdf):\n",
        "\n",
        "Daw, N. D. and Tobler, P. N. (2014) Value learning through Reinforcement: The Basics of Dopamine and Reinforcement Learning. Chapter 15, Neuroeconomics (2nd edition), edited by Glimcher, P. W. and Fehr, E.\n",
        "\n",
        "Try varying $\\alpha$ and replotting the weights to see how it affects the weight assigned to the history of outcomes.\n",
        "\n",
        "Last but not least, a bit of maths. Starting with equations 1 and 2, see if\n",
        "you can derive equation 6, by substituting terms from the previous trial's equation for $p_t$. (This is the hardest exercise we've asked you to do today. If you manage to complete it, well done! If not, don't worry we will reveal how to do it in session 2)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "practical",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
