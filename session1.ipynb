{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN0OKdOSQ1cd"
      },
      "source": [
        "**Course organisers**\n",
        "\n",
        "Jan Grohn (jan.grohn@psy.ox.ac.uk), Miriam Klein-Flügge (miriam.klein-flugge@psy.ox.ac.uk)  \n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Many experiments in learning and decision making require the estimation of values and how these change over time. Learning which stimuli and actions have higher value allows humans and other animals to select which stimuli to approach or avoid, and which actions to take.\n",
        "\n",
        "Some of the most commonly used models of learning and reinforcement were first developed by **Bush and Mosteller** in the 1950s, and further elaborated on by **Rescorla and Wagner** in the 1970s.\n",
        "\n",
        "They have since developed into the field of **Reinforcement Learning** in computer science.\n",
        "\n",
        "*Our aims for today’s session are:*\n",
        "\n",
        "1.  get to grips with Python, which we’ll be using throughout the four practicals for modelling and analyzing data\n",
        "2.  set up an experimental paradigm of learning and decision making, which we can use to test how model parameters are affected by manipulating subjects’ stress levels\n",
        "3.  code a simple reinforcement learning model that can be used to explain behaviour in this task\n",
        "4.  understand how changing parameters in this model affects its behaviour\n",
        "\n",
        "Parts of the session where you’re being asked to do something are indicated with an arrow (→).\n",
        "\n",
        "Use the text cells in this notebook to type in your answers.\n",
        "\n",
        "Note  that  the  final  section  (marked  ***)  is  a  slightly  harder  section.  It’s  not  essential  to understand  it  to  progress  with  the  later  sessions  in  the  course.  However,  it  provides  a theoretical foundation for the analyses that you’ll do in later practicals and can be a fun\n",
        "exercise to do!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnNeIN-pSq6z"
      },
      "source": [
        "#Section 1: Getting started with Python and Colab notebooks\n",
        "\n",
        "This notebook is supposed to run in a hosted colab. While it is also possible to download the notebook and host it locally, we do not recommend this (and if you try to run it locally you probably have to change some of the code below as it won't run out of the box).\n",
        "\n",
        "To ensure that the edits you make to your copy of the notebook are being saved, you can click on *File > Save a copy in Drive* in the top left corner.\n",
        "\n",
        "To run a code cell in this notebook, press Shift + Enter, or by clicking on the play symbol that appears when you hover over the code cell with your cursor. Run the next code cell now, which should produce some text output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pp2tkq3d_bV"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print('hello hello')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx6ZUxF_ftRc"
      },
      "source": [
        "You can open or close code cells with a title (such as the code cell below) by double clicking on the title, by clicking the ►/▼ symbol to the left of the title, or by clicking the 'Show code' hyperlink."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rU9Ps-GAf7eT"
      },
      "outputs": [],
      "source": [
        "#@title ### This cell has some hidden code\n",
        "\n",
        "print('hello again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljv-EAA6jsHn"
      },
      "source": [
        "If you encounter a Python function you don't know yet or you would like to know more about, you can type `?` followed by the function name (e.g. `?print`) into a new code cell. A new code cell can be created by selecting *Insert > Code cell* from the menu bar in the top left, or by hovering between existing cells with your cursor and clicking the *+ Code* icon that appears.\n",
        "\n",
        "Create a code cell below this cell and view the help text for the `print` function now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duJGeDhP9DE-"
      },
      "source": [
        "You can view a **Table of Contents** by clicking the icon in the top left with the three dots next to three lines.\n",
        "\n",
        "In Python we can load in functions that other people wrote so that we don't have to code up everything from scratch. These sets of functions are organised in *libraries*, and they are loaded into Python with the `import` command, followed by the name of the library. We can also specify a different name for the library to call it by internally using the `as` statement. For example, `import numpy as np` loads in the `numpy` library, which we can then access using the `np` command. We can also load in a specific funtion from a library by using the `from` statement.\n",
        "\n",
        "Run the next code cell now that will load in some libraries that we will be using throughout the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeEaQjFUYxCP"
      },
      "outputs": [],
      "source": [
        "#@title ## Import libraries and set global parameters\n",
        "\n",
        "# numpy is a libarary used to do all kinds of mathematical operations\n",
        "import numpy as np\n",
        "\n",
        "# this allows us to make interactive figures\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# seed the random number genrator\n",
        "rng = np.random.default_rng(12345)\n",
        "\n",
        "# load in some custom functions for this block practical\n",
        "!rm -r *\n",
        "!git clone https://github.com/jangrohn/ComputationalmodelingBlockPractical\n",
        "!cp -R ComputationalmodelingBlockPractical/session1/ session1\n",
        "!rm -rf ComputationalmodelingBlockPractical\n",
        "from session1 import plotting # type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWiW8-gRvDiz"
      },
      "source": [
        "The last couple of lines of code in the previous cell load in some custom functions we wrote for this block practical, which we will be using throughout this session and also the next sessions. If you're interested in examining the code further, you can find them at https://github.com/JanGrohn/ComputationalModelingBlockPractical or by clicking the folder icon in the sidebar to the left and look through the scripts there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWLm4LvgS-KE"
      },
      "source": [
        "#Section 2: Running a simple experiment\n",
        "\n",
        "You can try out the task we will be analysing in this block practical at  https://jangrohn.github.io/volatility_study/. Play through the task as best as you can to get a feel for the study. At the beginning of the task, initially choose one of the two conditions randomly but if you have the time also restart the task and play the other condition afterwards. The task doesn't save any of your data, but you will have an option to download it once you're finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VFNvxBrTuQ9"
      },
      "source": [
        "#Section 3: Coding a simple reinforcement learning model in Python, and understanding the effects of varying learning rates\n",
        "\n",
        "*How did you learn whether green or orange was more likely to be rewarded?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJWmrJLkUGfy"
      },
      "source": [
        "→ type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rKNVSJsUHod"
      },
      "source": [
        "*How did you weigh this up against the number of points available when making your choices?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YljKNlPaUL5K"
      },
      "source": [
        "→ type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ9VSJT_I_41"
      },
      "source": [
        "*What do you think was the difference between condition 1 and condition 2 in the task?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AJzreFVJFrJ"
      },
      "source": [
        "→ type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-EC_2RUUPzH"
      },
      "source": [
        "*Why do you think we included points – rather than simply letting participants learn which option has a higher reward probability?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YFsTI_6UToa"
      },
      "source": [
        "→ type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4wSVvO0Ub6H"
      },
      "source": [
        "Computational modelling tries to describe these questions using mathematical equations. We can then provide a *quantitative* answer to each of them, using *parameters* from the model.\n",
        "\n",
        "*What do you think might be the difference between a model parameter and a\n",
        "variable? Discuss your answer with your partner.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsIkhc-OUoaM"
      },
      "source": [
        "→ type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAMps7idUyer"
      },
      "source": [
        "Reinforcement  learning  models  provide  a  way  of  tracking  the  probabilities  of  different outcomes when different actions are taken.\n",
        "\n",
        "In this task, one of the key problems is to estimate *the probability that green is to be rewarded* on each trial. (Note that this is the *same* as 1 minus the probability that blue will be rewarded – so we only need to track one probability).\n",
        "\n",
        "We can learn this probability by calculating a *prediction error* on each trial:\n",
        "\n",
        "$$\n",
        "\\underbrace{\\delta_t}_\\textrm{prediction error} = \\underbrace{o_t}_\\textrm{outcome} - \\underbrace{p_t}_\\textrm{model prediction} \\tag{Equation 1}\n",
        "$$\n",
        "\n",
        "where $\\delta_t$ is the prediction error on trial $t$, $o_t$ is the outcome (1 if green was rewarded, 0 if blue was rewarded) on that trial, and $p_t$ is the current estimate of the probability that green will be rewarded (this is called `probOpt1` in the Python code). Have a think about what a prediction error might look like on a trial that does give reward and on a trial where no reward is obtained. What sign does it take in each case?\n",
        "\n",
        "We then use this prediction error to *update our expectation* of how likely it is that green will be rewarded in the future.\n",
        "\n",
        "$$\n",
        "\\underbrace{p_{t+1}}_\\textrm{new prediction} = \\underbrace{p_t}_\\textrm{old prediction} + \\underbrace{\\alpha \\delta_t}_\\textrm{scaled prediction error} \\tag{Equation 2}\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a parameter called the *learning  rate*, whose value is $0 < \\alpha \\leq 1$. In your head, simulate different scenarios of this equation with a large (1) or small (0.1) learning rate and a positive or negative prediction error. What does the update look like, how does it differ?\n",
        "\n",
        "The learning rate sets the *speed* at which the model learns from previous experience. Let’s explore directly in Python what happens when we vary the learning rate.\n",
        "\n",
        "## Learning rates, and how they affect probability learning\n",
        "\n",
        "The next code cell sets up, and plots, a simple ‘reward schedule’ where the true probability of green being rewarded is fixed at 0.8. Read through this code and try to understand what it is doing.\n",
        "\n",
        "Once you have read through the code, run it to produce the figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8u5lMFUUGAk"
      },
      "outputs": [],
      "source": [
        "#@title ### Learning with a fixed schedule\n",
        "\n",
        "def generate_schedule(trueProbability, rng = rng):\n",
        "  '''\n",
        "  Returns if option 1 (1) or option 2 (0) is rewarded on a trial\n",
        "\n",
        "    Parameters:\n",
        "        trueProbability(float array): The probability with which option 1 is\n",
        "          rewarded on each trial\n",
        "        rng (numpy random number generator, defaults to rng)\n",
        "\n",
        "    Returns:\n",
        "        opt1rewarded(int array): 1 if option 1 is rewarded on a trial, 0 if\n",
        "          option 2 is rewarded on a trial\n",
        "  '''\n",
        "  # We'll simulate whether opt 1 was rewarded on every trial. For each trial, we\n",
        "  # first generate a uniformly distributed random number between 0 and 1.\n",
        "  randomNumbers = rng.random(len(trueProbability))\n",
        "\n",
        "  # The trial is rewarded if that number is smaller than trueProbability, and\n",
        "  # unrewarded otherwise.\n",
        "  opt1Rewarded = randomNumbers < trueProbability\n",
        "\n",
        "  # We return the outcome of this comparison (which is either True or False for\n",
        "  # each trial) an an integer (which is 0 or 1 for each trial).\n",
        "  return opt1Rewarded.astype(int)\n",
        "\n",
        "# this is the true probability that green is rewarded\n",
        "fixedProb = 0.8\n",
        "\n",
        "# this is the number of trials\n",
        "nTrials = 200\n",
        "\n",
        "# reward probability on each trial\n",
        "trueProbability = np.ones(nTrials, dtype = float) * fixedProb\n",
        "\n",
        "# generate outcomes on each trial\n",
        "opt1Rewarded = generate_schedule(trueProbability)\n",
        "\n",
        "# visualise which option was rewarded on each trial\n",
        "plotting.plot_schedule(opt1Rewarded, trueProbability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjnZma4urfqS"
      },
      "source": [
        "The dots are at 1 every time that green is rewarded, and at 0 every time that blue is rewarded. The black dotted line is the true probability of reward (which the subject doesn’t know in the experiment).\n",
        "\n",
        "The function RL_model defined in the next cell takes as its input: whether green is rewarded on each trial (`opt1rewarded`), what the model’s learning rate $\\alpha$ is set to, and what the starting probability on the first trial is ($p_1$). It tries to return `probOpt1`, the probability of green being rewarded, as its output. However, the final two equations in the function\n",
        "haven’t been completed. Open the next cell and complete the missing lines of code.\n",
        "\n",
        "Once you’ve done this, you should be able to run this cell without receiving any errors, and the figure should now have a red trace that approaches the true probability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ks3qNcHTuAp"
      },
      "outputs": [],
      "source": [
        "#@title ### Simulating the RL model\n",
        "def RL_model(opt1Rewarded, alpha, startingProb = 0.5):\n",
        "  '''\n",
        "  Returns how likely option 1 is rewarded on each trial.\n",
        "\n",
        "    Parameters:\n",
        "        opt1rewarded(bool array): True if option 1 is rewarded on a trial, False\n",
        "          if option 2 is rewarded on a trial.\n",
        "        alpha(float): fixed learning rate, greater than 0, less than/equal to 1\n",
        "        startingProb(float): starting probability (defaults to 0.5).\n",
        "\n",
        "    Returns:\n",
        "        probOpt1(float array): how likely option 1 is rewarded on each trial\n",
        "          according to the RL model.\n",
        "  '''\n",
        "\n",
        "  # check that alpha has been set appropriately\n",
        "  assert alpha > 0, 'Learning rate (alpha) must be greater than 0'\n",
        "  assert alpha <= 1,'Learning rate (alpha) must be less than or equal to 1'\n",
        "\n",
        "  # check that startingProb has been set appropriately\n",
        "  assert startingProb >= 0, 'startingProb must be greater or equal than 0'\n",
        "  assert startingProb <= 1, 'startingProb must be less than or equal to 1'\n",
        "\n",
        "  # calculate the number of trials\n",
        "  nTrials = len(opt1Rewarded)\n",
        "\n",
        "  # pre-create some vectors we're going to assign into\n",
        "  probOpt1 = np.zeros(nTrials, dtype = float)\n",
        "  delta    = np.zeros(nTrials, dtype = float)\n",
        "\n",
        "  # set the first trial's prediction to be equal to the starting probability\n",
        "  probOpt1[0] = startingProb\n",
        "\n",
        "  # students, complete this code to finish the reinforcement learning model\n",
        "  for t in range(nTrials-1): # loop over trials\n",
        "    delta[t] =      # COMPLETE THIS LINE using opt1Rewarded, probOpt1 and equation 1\n",
        "    probOpt1[t+1] = # COMPLETE THIS LINE  using probOpt1, delta, alpha and equation 2\n",
        "\n",
        "\n",
        "  return probOpt1\n",
        "\n",
        "# this defines the model's estimated pronbabilty on the very first trial\n",
        "startingProb = 0.5\n",
        "\n",
        "# this is the model's learning rate\n",
        "alpha = 0.05\n",
        "\n",
        "# run the RL model\n",
        "probOpt1 = RL_model(opt1Rewarded, alpha, startingProb)\n",
        "\n",
        "plotting.plot_schedule(opt1Rewarded, trueProbability, probOpt1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7m8qijvtwIf"
      },
      "source": [
        "Now  try  playing  around  with  the  three  parameters `trueProb` `startingProb`, and `alpha`. Particularly try to understand the effects of varying `alpha`. What are the advantages of having a low $\\alpha$? What are the advantages of having a high $\\alpha$? If you set $\\alpha$ to 1, how does the model behave? You can change the value using the sliders below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzLmPt6oAiDX"
      },
      "outputs": [],
      "source": [
        "plotting.plot_interactive_RL_model(opt1Rewarded, trueProbability, RL_model, generate_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uijr-Ym3xEg2"
      },
      "source": [
        "→ type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14dSaEmzyQA8"
      },
      "source": [
        "## Using a reversal schedule\n",
        "In the experiment you performed earlier, the reward probability isn’t fixed, but it reverses at  various  points  during  the  experiment  –  so  at  some  points  green  is  more  likely  to  be rewarded, but at other points blue is more likely to be rewarded.\n",
        "\n",
        "Let’s now generate such a schedule during which reversals take\n",
        "place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iHpftBxnCB83"
      },
      "outputs": [],
      "source": [
        "#@title ### Regenerating the schedule and the RL model\n",
        "\n",
        "# now we use a schedule with some reversals\n",
        "trueProbability = np.concatenate((np.ones(25,  dtype = float)*0.25,\n",
        "                                  np.ones(25,  dtype = float)*0.75,\n",
        "                                  np.ones(25,  dtype = float)*0.25,\n",
        "                                  np.ones(25,  dtype = float)*0.75,\n",
        "                                  np.ones(100, dtype = float)*0.25))\n",
        "\n",
        "opt1Rewarded = generate_schedule(trueProbability)\n",
        "\n",
        "plotting.plot_interactive_RL_model(opt1Rewarded, trueProbability, RL_model, generate_schedule, change_trueProb = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_KpGykJ06c0"
      },
      "source": [
        "Change the parameters above to see whether your reinforcement learner can keep track of this changing probability.\n",
        "\n",
        "In the above schedule, there are periods where the reward environment is volatile (it reverses quite frequently) and other periods where the reward environment is stable (it doesn't change very frequently). Our hypothesis  is  that  people  adjust  their  learning  rates  depending  upon  the  stability  of  the environment.\n",
        "\n",
        "Let's assume that subjects want to get their estimated probability as close to the true probability as possible. Based upon what you learnt about varying $\\alpha$, in which environment might it be helpful to have a lower $\\alpha$? In which environment might it be helpful to have a higher $\\alpha$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK6ui_Qi145D"
      },
      "source": [
        "→ type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDmHw4CKlP4h"
      },
      "source": [
        "The reasoning behind why you might need to have a different $\\alpha$ in different situations is discussed more fully in the following paper:\n",
        "\n",
        "Behrens, T. E. J., Woolrich, M. W., Walton, M. E., & Rushworth, M. F. S. (2007). Learning the  value  of  information  in  an  uncertain  world,  Nature  Neuroscience  10(9),  1214–1221. http://doi.org/10.1038/nn1954"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jZjHSuV18zx"
      },
      "source": [
        "#***Section 4: Understanding learning rates as weighted sums\n",
        "\n",
        "The key insight here is that the learning rate, $\\alpha$, determines how the currently estimated probability on the next trial, $p_{t+1}$, is influenced by the past history of trials. We can think about  this in another way. Look back at equations 1 and 2. $p_{t+1}$ depends upon the outcome of the\n",
        "most recent trial, $o_t$, but also the last trial’s probability estimate, $p_t$. However, $p_t$ could itself be written in terms of the probability of the previous trial’s outcome, $p_{t-1}$, and $p_{t-1}$. In turn $p_{t-1}$ could\n",
        "be written in terms of $o_{t-2}$ and $p_{t-2}$. And so on.\n",
        "\n",
        "In short, it becomes possible, when you are at trial $T$, to think of $p_{T+1}$ as a weighted sum of all the previous outcomes that the model experienced:\n",
        "\n",
        "$$\n",
        "p_{T+1} = (1-\\alpha)^T p_1 + \\sum^T_{t=1} w_t o_t \\tag{Equation 3}\n",
        "$$\n",
        "\n",
        "Where $p_1$ is the starting probability (this becomes less and less important as we get further away from it, as $(1-\\alpha)^T$ shrinks to zero), and $w_t$ is the amount of weight given to the outcome on trial $t$ on the current trial:\n",
        "\n",
        "$$\n",
        "w_t = \\alpha(1-\\alpha)^{T-t} \\tag{Equation 4}\n",
        "$$\n",
        "\n",
        "$T$ is the current trial number, and so $T-t$ indexes how many trials into the past we are looking. In the final exercise, we see if you can derive this equation, step- by-step.\n",
        "\n",
        "What does $w_t$ actually look like? This is explored in the next cell, which plots equation 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk6Rp5DcFnmt"
      },
      "outputs": [],
      "source": [
        "plotting.plot_RL_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPRC8pDX67Iz"
      },
      "source": [
        "This plot is also discussed at the beginning of the following book chapter (which is available as a PDF online at http://www.princeton.edu/~ndaw/dt.pdf):\n",
        "\n",
        "Daw, N. D. and Tobler, P. N. (2014) Value learning through Reinforcement: The Basics of Dopamine and Reinforcement Learning. Chapter 15, Neuroeconomics (2nd edition), edited by Glimcher, P. W. and Fehr, E.\n",
        "\n",
        "Try varying $\\alpha$ and replotting the weights to see how it affects the weight assigned to the history of outcomes.\n",
        "\n",
        "Last but not least, a bit of maths. Starting with equations 1 and 2, see if\n",
        "you can derive equation 4, by substituting terms from the previous trial's equation for $p_t$. (This is the hardest exercise we've asked you to do today. If you manage to complete it, well done! If not, don't worry we will reveal how to do it in session 2)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
